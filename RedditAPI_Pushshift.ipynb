{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/matth/OneDrive/Documents/OAuth2.json\n"
     ]
    }
   ],
   "source": [
    "#The purpose of this script is to analyze the relationship between mentions of a company on Reddit and the stock price\n",
    "#it was created to help quantify the meme stock movement in 2021, specifically with respect to GameStop (GME)\n",
    "#GameStop is hardcoded as the company being analyzed however it could be leveraged for any company\n",
    "#It was creating using Pushshift, which no longer appears to work because Reddit decided to make it's API less accessible\n",
    "#This code creates the dataset that feeds this \"GME Mentions\" dashboard so the final result can still be viewed here: https://public.tableau.com/views/GMEMentions/GMEStockvs_RedditMentions?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link\n",
    "\n",
    "#References that supported the creation of this code:\n",
    "#https://www.jcchouinard.com/how-to-use-reddit-api-with-python/\n",
    "#https://github.com/pushshift/api#using-the-subreddit-aggregation \n",
    "#https://reddit-api.readthedocs.io/en/latest/\n",
    "#https://github.com/Watchful1/Sketchpad/blob/master/postDownloader.py\n",
    "\n",
    "import plotly\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import dateutil\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Designate a filepath to save results to as an option to save time\n",
    "output_path = os.environ['output_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The purpose of this function is to pull Reddit pushshift data through the API for one API call\n",
    "\n",
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/{data_type}/\"\n",
    "    payload = kwargs\n",
    "    request = requests.get(base_url, params=payload)\n",
    "    \n",
    "    #Cleaning things up to make move to a data frame\n",
    "    data = request.json()\n",
    "    \n",
    "    #Return the final result\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m get_pushshift_data(data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[0;32m     19\u001b[0m                       q\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m     20\u001b[0m                       after\u001b[38;5;241m=\u001b[39mduration,\n\u001b[0;32m     21\u001b[0m                       size\u001b[38;5;241m=\u001b[39msize,\n\u001b[0;32m     22\u001b[0m                       before\u001b[38;5;241m=\u001b[39mbefore,\n\u001b[0;32m     23\u001b[0m                       subreddit\u001b[38;5;241m=\u001b[39msubreddit)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#get the next batch of data\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more data on loop \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(counter))\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:2491\u001b[0m, in \u001b[0;36mDataFrame.from_records\u001b[1;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[0;32m   2489\u001b[0m     arr_columns \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2491\u001b[0m     arrays, arr_columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns)\n\u001b[0;32m   2492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coerce_float:\n\u001b[0;32m   2493\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:811\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_arrays\u001b[39m(\n\u001b[0;32m    794\u001b[0m     data, columns: Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dtype: DtypeObj \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[ArrayLike], Index]:\n\u001b[0;32m    796\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m    Return list of arrays, columns.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03m    Ensures that len(result_arrays) == len(result_index).\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    813\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m                 \u001b[38;5;66;03m# i.e. numpy structured array\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#The purpose of this section is to loop and incrementally pull all the Reddit data from pushshift and consolidate into one dataframe\n",
    "\n",
    "data_type=\"submission\"            # give me comments, use \"submission\" to publish something\n",
    "query=\" GME|Gamestop|$GME|\"       # Add your query\n",
    "duration=\"12d\"                    # Select the timeframe. Epoch value or Integer + \"s,m,h,d\" (i.e. \"second\", \"minute\", \"hour\", \"day\")\n",
    "size=1000                         # maximum comments\n",
    "sort_type=\"score\"                 # Sort by score (Accepted: \"score\", \"num_comments\", \"created_utc\")\n",
    "sort=\"desc\"                       # sort descending\n",
    "aggs=\"subreddit\"                  #\"author\", \"link_id\", \"created_utc\", \"subreddit\" #seems to be disabled\n",
    "subreddit='wallstreetbets'\n",
    "before = \"\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "counter = 0\n",
    "\n",
    "#we are going to loop until there is no more data so just do a while loop and break it when we run out of data below\n",
    "while True:\n",
    "    data = get_pushshift_data(data_type=data_type,\n",
    "                          q=query,\n",
    "                          after=duration,\n",
    "                          size=size,\n",
    "                          before=before,\n",
    "                          subreddit=subreddit)\n",
    "\n",
    "    #get the next batch of data\n",
    "    new_df = pd.DataFrame.from_records(data.get(\"data\"))\n",
    "    \n",
    "    if new_df.shape[0] == 0:\n",
    "        print(\"No more data on loop \" + str(counter))\n",
    "        break\n",
    "        \n",
    "    # take the final row (oldest entry)\n",
    "    row = new_df.iloc[len(new_df)-1]\n",
    "    \n",
    "    #Create a variable so we know to pull data after this time in the next batch\n",
    "    duration = row['created_utc']+1\n",
    "    \n",
    "    # append new_df to data\n",
    "    df = df.append(new_df, ignore_index=True)\n",
    "    \n",
    "    #Slow down so we don't get rate limited\n",
    "    time.sleep(5)\n",
    "\n",
    "    #increse the counter and let the user know what number we are on\n",
    "    counter = counter + 1\n",
    "    print(counter)\n",
    "    \n",
    "#df['Created_Date'] = ''\n",
    "\n",
    "#Loop to create the created_Date column, couldn't get this to work in one line\n",
    "#for index, row in df.iterrows():\n",
    "    \n",
    "    #print(row['created_utc'])\n",
    "#    row['Created_Date'] = datetime.fromtimestamp(row['created_utc']).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#This adds the created date as an actual date we can use\n",
    "df['Created_Date'] = df.apply(lambda row: pd.Timestamp(datetime.fromtimestamp(row['created_utc']).strftime(\"%Y-%m-%d\")), axis = 1) \n",
    "\n",
    "#This creates a new dataframe that groups the total records by date, using title as an anchor assuming that will always\n",
    "#be present\n",
    "df_Date = df.groupby(['Created_Date'])['title'].count().to_frame()\n",
    "\n",
    "#Similar to sthe stock df below this makes the created_date an actual field to reference\n",
    "df_Date = df_Date.reset_index()\n",
    "\n",
    "#write the data to a csv for analysis as needed\n",
    "df.to_csv(output_path + \"Reddit_GMEAllData2.csv\")\n",
    "df_Date.to_csv(output_path + \"Reddit_GMESummary2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pull the stock prices through yfinance\n",
    "\n",
    "#Pull GME data\n",
    "stock = yf.Ticker('GME')\n",
    "\n",
    "#start at 11/23 because that's when we have Reddit data through\n",
    "prices = stock.history(start=\"2021-06-28\")\n",
    "\n",
    "#I don't fully know what this does but it allows us to chart and moves the Date field from metadata to a field we can reference\n",
    "prices = prices.reset_index()\n",
    "\n",
    "#write the prices down to a csv file for analysis as needed\n",
    "prices.to_csv(output_path + \"Reddit_GMEPrices2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will merge the stock and Reddit data and create moving 3 day averages\n",
    "\n",
    "rollwin = 3\n",
    "\n",
    "#Before we merge, it will be easiest to do the moving average on stock prices first because the dates are more uneven\n",
    "#as there are reddit mentions daily\n",
    "#This would work better as it's own function that both areas are referencing rather than doing this process twice\n",
    "prices['pandas_SMA_3_P'] = prices.iloc[:,1].rolling(window=rollwin).mean()\n",
    "prices['SMA_pct_P'] = 0\n",
    "\n",
    "#calculate the x-day moving average based on the rollwin variable\n",
    "for ind in prices.index:\n",
    "    if ind > rollwin-1:\n",
    "        prices.loc[ind,'SMA_pct_P'] = prices.loc[ind,'Close']/prices.loc[ind-1,'pandas_SMA_3_P']-1\n",
    "\n",
    "#Merge the stock and reddit data - outer join on date since we have reddit data daily but stock data only on trading days\n",
    "#Combine the stock prices and the reddit mentions\n",
    "df_all = df_Date.merge(prices,how='outer',left_on='Created_Date',right_on='Date')\n",
    "\n",
    "#Create a new column to hold our combined date\n",
    "df_all['EffDt'] = 0\n",
    "\n",
    "#Populate the new date column with either the Created_Date or the Date from the stock price data to get one consolidated date field\n",
    "for i in df_all.index:\n",
    "    if pd.isnull(df_all.loc[i,'Created_Date']):\n",
    "        df_all.loc[i,'EffDt'] = df_all.loc[i,'Date']\n",
    "    else:\n",
    "        df_all.loc[i,'EffDt'] = df_all.loc[i,'Created_Date']\n",
    "\n",
    "#Sort on EffDt\n",
    "df_all = df_all.sort_values('EffDt')\n",
    "        \n",
    "#Keep only the columns we need\n",
    "df_all = df_all[['EffDt','title','Close','Volume','pandas_SMA_3_P','SMA_pct_P']]\n",
    "df_all = df_all.rename(columns = {'title':'Reddit_Mentions'})\n",
    "\n",
    "#********This is creating the X day moving average and creating the percentage change******************\n",
    "df_all['pandas_SMA_3_RM'] = df_all.iloc[:,1].rolling(window=rollwin).mean()\n",
    "df_all['SMA_pct_RM'] = 0\n",
    "\n",
    "#I'm looping this because I want to compare a given row with the previous row's moving 3 day average.\n",
    "#If we do a 3 day moving average including the current row then an increase or decrease for the current row might dilute the\n",
    "#impact of a change\n",
    "for ind in df_all.index:\n",
    "    if ind > rollwin-1:\n",
    "        df_all.loc[ind,'SMA_pct_RM'] = df_all.loc[ind,'Reddit_Mentions']/df_all.loc[ind-1,'pandas_SMA_3_RM']-1\n",
    "\n",
    "df_all.to_csv(output_path + \"Reddit_GMECombinedV2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a visual of the data\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=prices[\"Date\"], y=prices[\"Close\"], name=\"GME data\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_Date[\"Created_Date\"], y=df_Date[\"title\"], name=\"WSB mentions data\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"GME Price vs. Reddit Mentions\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>primary</b> GME Stock Price\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>secondary</b> WSB Mentions\", secondary_y=True)\n",
    "\n",
    "#show the chart\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
